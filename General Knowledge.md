#### Latency is about how fast a system responds to a single request while throughput is about how many requests a system can handle every second.

Latency is usually inversely proportional to throughput i.e. optimizing latency results in killing throughput and vice versa. For example, caching in database layer for better latency results in lower memory available for other tasks, reducing the amount of what a system can handle. Finding the right balance is very important.
